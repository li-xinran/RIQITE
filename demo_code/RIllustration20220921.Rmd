---
title: "Data analysis and power analysis using the developed package RIQITE"
author: "Devin Caughey, Allan Dafoe, Xinran Li, and Luke Miratrix"
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

---
bibliography: ["permutation.bib"]
biblio-style: "apalike"
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Dropbox/Own/Projects/Quantile Effects/code2021Aug/R illustration")
library(dplyr)
library(ggplot2)
library( RIQITE )
library( tidyverse )
set.seed(1)
nperm = 1000
```

# Install the RIQITE package
We first install the package from Github. 
The github page of the package (<https://github.com/li-xinran/RIQITE>) contains main functions with detailed explanation in R documentation, as well as a simple illustrating example. 
Below we provide the code used to analyze the data sets in the paper. 

Install via `devtools`, and then load:
```{r, message=FALSE, eval=FALSE}
devtools::install_github("li-xinran/RIQITE")
library(RIQITE)
```


# Testing monotonicity of an instrumental variable

We get the data from @BlackEtAl11a, in which Table 1 cross-tabulates month of birth and school entry age (early/on-time/late) in terms of proportions and Table 3 reports the total number of subjects born in January or December (the "discontinuity subsample"): 104,023. We round the total number of subjects to 104,000, and, similar to @FioriniStevens14a, we assume there are equal numbers of subjects born in January and December.
Based on these summary statistics, we generate the individual-level data as follows:

```{r get the data, message=FALSE}
x <- matrix(0, 2, 3, dimnames=list(Month=c("Dec", "Jan"), 
                                   Entry=c("Early", "On-Time", "Late")))
n <- 104000
x["Dec", "On-Time"] <- .85
x["Dec", "Late"] <- .15
x["Jan", "Early"] <- .10
x["Jan", "On-Time"] <- .90
df <- data.frame(BirthMonth = rep(c("Dec", "Jan"), each=n/2), 
                 EntryTiming = c(rep("On-Time", n/2 * .85), rep("Late", n/2 * .15), 
                                 rep("Early", n/2 * .10), rep("On-Time", n/2 * .90)))
df <- df %>% mutate(EntryAge = NA,
        EntryAge = ifelse(BirthMonth == "Dec" & EntryTiming == "On-Time", 6.75, EntryAge),
        EntryAge = ifelse(BirthMonth == "Dec" & EntryTiming == "Late", 7.75, EntryAge),
        EntryAge = ifelse(BirthMonth == "Jan" & EntryTiming == "On-Time", 23/3, EntryAge),
        EntryAge = ifelse(BirthMonth == "Jan" & EntryTiming == "Early", 20/3, EntryAge))
data <- df[sample(c(1:n)), ] # randomly permute the ordering of units
table(data$BirthMonth, data$EntryTiming)
```

The following figure shows the empirical distribution functions of the school-entry age of December-born and January-born subjects. From the figure, most children started school at an older age if they were born in January rather than December.

```{r, fig.height = 3, fig.width = 3, fig.align = "center"}
ggplot(data, aes(EntryAge, color = BirthMonth, linetype=BirthMonth)) +
  stat_ecdf(geom="step", show.legend=FALSE, size=1.3) +
  annotate("text", x=c(7.2, 7.2), y=c(.145, .81), 
           label=c("January", "December")) +
  scale_x_continuous(breaks=seq(6, 8, .2)) + 
  theme_bw() + 
  theme( plot.background = element_blank(), 
         panel.grid.major = element_blank(), 
         panel.grid.minor = element_blank(),
         plot.title=element_text(hjust=.5 ) ) + 
  labs(x="School-Entry Age (Years)", y="Probability") 
```

We first perform the standard regression analysis assessing the relation between the instrument (birthday) and the school-entry age. The first-stage relationship is incredibly strong, with an average effect of $-0.667$ years and an F statistic of 106,256. This would conventionally be considered persuasive evidence of a valid instrument.

```{r F test}
lm.fit <- lm(EntryAge ~ (BirthMonth=="Dec"), data)
cat(paste( 
  "esimated average effect is", round( as.numeric(lm.fit$coefficients[2]), digits = 3), 
  "and F statistic is", round( as.numeric( summary(lm.fit)$fstatistic[1] ), digits = 0), "\n" 
  ))
```

We then perform the randomization test for the bounded null $H_{\le 0}$ on the monotonicity of the instrument. Specifically, we consider three randomization tests using the difference-in-means statistic, Wilcoxon rank sum statistic and Stephenson rank sum statistic with the parameter $s=10$, respectively. We also conduct the classical Student's $t$-test. These produce Table 1 in the paper.

```{r p value for bounded null, cache=TRUE}
data$Z = as.numeric( data$BirthMonth == "Dec" )
data$Y = data$EntryAge
n <- length(data$Z)
m <- sum(data$Z)
nperm <- 10^4 # number of permutations for Monte Carlo approximation

# Generate a sample of permutations
Z.perm <- assign_CRE(n, m, nperm)

# difference-in-means
pval.dim <- pval_bound(Z=data$Z, Y=data$Y, c=0, method.list=list(name = "DIM"), 
                     alternative = "greater", Z.perm=Z.perm, impute="control")
ci.dim <- ci_bound(Z=data$Z, Y=data$Y, alternative="greater", method.list=list(name="DIM"), 
                        Z.perm=Z.perm, impute="control", alpha=0.10)

# Wilcoxon rank sum
pval.wilc <- pval_quantile(Z=data$Z, Y=data$Y, k=n, c=0, alternative = "greater",
                           method.list=list(name="Stephenson", s=2), Z.perm=Z.perm )
ci.wilc <- ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
                method.list=list(name="Stephenson", s=2), Z.perm=Z.perm,  alpha=0.10)

# Stephenson rank sum
pval.steph10 <- pval_quantile(Z=data$Z, Y=data$Y, k=n, c=0, alternative = "greater", 
                              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm )
ci.steph10 <- ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.10)

# classical Student's t-test
ttest = t.test(data$Y[data$Z==1], data$Y[data$Z==0], alternative = "greater")
pval.ttest = ttest$p.value
ci.ttest = round( ttest$conf.int[1], digits = 3)

# result for Table 1 in the paper
result = data.frame(test=c("diff-in-means", "Wilcoxon", "Stephenson 10", "t test"),
                    pval=c(pval.dim, pval.wilc, pval.steph10, pval.ttest), 
                    lower = c(ci.dim, ci.wilc$lower, ci.steph10$lower, ci.ttest))
knitr::kable( result )
```

Finally, we infer the effect range. The result shows that the range of the effect of birth month on school-entry age is at least 1 year, indicating significant individual effect heterogeneity. 
```{r}
tau.max.lower = ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.05)$lower
tau.min.upper = ci_quantile(Z=data$Z, Y=data$Y, k.vec = 1, alternative = "less", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.05)$upper
print(paste( "lower confidence limit for effect range is", 
             max(0, tau.max.lower - tau.min.upper) ))
save.image("BirthSchoolAge20220921.RData") # save the analysis result
```



# Evaluating the effectiveness of professional development

We next evaluate a teacher professional development RCT where treated teachers were given a professional development course on electric circuits.
We get the data from @HSMHSD10.
The cleaned data, as described in the paper, is part of the package.
We load it and estimate the average treatment effect (ATE) via linear regression:

```{r get the professional development data, message=FALSE}
data( electric_teachers )

mod <- lm(gain~TxAny, data = electric_teachers)
est_ATE <- round(mod$coefficients[2], digits = 2)
est_ATE
```

The following figure shows the histograms of the observed gain scores in the treatment and control groups. 
```{r, fig.height = 4, fig.width = 3.2, fig.align = "center"}
hist(electric_teachers$gain[electric_teachers$TxAny==1], 
     xlim = range(electric_teachers$gain), freq = FALSE, ylim = c(0, 0.065), 
     col = "grey", border=FALSE, breaks = 20,
     main = NULL, xlab = "gain score")
hist(electric_teachers$gain[electric_teachers$TxAny==0], 
     freq = FALSE, add = TRUE, breaks = 20, col = NULL)
```

We can compare the variation of the units in the treatment and control arms, which can be a clue to how much heterogeneity there might be.
We do this in effect size units, by first standardizing gain by the control-side variation.
```{r}
sd_0 = sd( electric_teachers$gain[ electric_teachers$TxAny == 0 ] )
electric_teachers$std_gain = electric_teachers$gain / sd_0
electric_teachers %>% group_by( TxAny ) %>%
    summarise( raw_mean = mean( gain ),
               mean = mean( std_gain ),
               var = var( std_gain ),
               max = max( std_gain ),
               min = min( std_gain ),
               raw_sd = sd( gain ),
               sd = sd( std_gain ) )
```

Under no correlation between $Y_i(0)$ and $\tau_i$, the standard deviation of the treatment impacts would be, using $var(Y_i(1)) = var(Y_i(0)) + var(\tau_i)$,
```{r}
sqrt( 1.12 - 1 )
```
I.e., around $0.35\sigma$, in effect size units.  This would be $0.35 \times sd_0$, or `r round(sqrt( 1.12 - 1 ) * sd_0,digits=1)` in the original scale.

An estimate of the minimum treatment effect possible is the largest control outcome minus the largest treated outcome, which is about $3.16 - -0.63 = `r 3.16 + 0.63`$.
The maximum, by contrast, is $5.06 - -2.22 = `r 5.06 + 2.22`.


## Determining an s-value for the test statistic

We plan on using a Stephenson's Rank Sum test, but need to determine an appropriate value of $s$.
We first conduct a sensitivity analysis across a range of scenarios to determine which $s$ have the highest amount of power for the  top 10 quantiles, the next 20 quantiles, and the next 40 quantiles.

To illustrate, we first run a check on best $s$ for a single scenario.
We first get the empirical distribution of the control outcomes and calculate a treatment effect (all in standardized units).
We then explore the case where the impacts are normally distributed, and negatively correlated with $Y_i(0)$, which could be the case for our data given our histogram above.

```{r run_single_s_explore, cache=TRUE}
cdat = filter( electric_teachers, TxAny == 0 )
ate = est_ATE / sd_0
R = 100
nperm = 10^3
s_list = c( 2, 3, 4, 5, 6, 8, 10, 15, 20 )
res <- explore_stephenson_s( s = s_list,
                      n = nrow( electric_teachers ),
                      Y0_distribution = cdat$std_gain,
                      tx_function = "rnorm", ATE = ate, 
                      tx_scale = 0.35, rho = -0.5,
                      nperm = nperm,
                      R = R, calc_ICC = FALSE,
                      targeted_power = FALSE, k.vec = (233-100):233 )
```

We get one row per targeted quantile and explored $s$, telling us how easy it was to detect effects at that quantile.
Here we see how our performance was on the 200th largest effect.
```{r}
filter( res, k == 200 )
```
We note $s$ of about 3 or 4 tends to give the highest bound on the impact (see `q_ci`). 

We next make a grid of different scenarios to explore:
```{r}
checks = expand_grid( TxVar = c( 0.25, 0.75 ),
                      tx_dist = c( "rexp", "rnorm" ),
                      rho = c( -0.5, 0, 0.5 ) )

# There is no treatment variation, nor correlation with Y0, when
# the tx impact is a constant.
checks = bind_rows( checks,
                    tibble( TxVar = 0, tx_dist = "constant", rho = 0 ) )
```

This gives `r nrow(checks)` scenarios. We use `pmap()` to run `explore_stephenson_s` across them.
The `explore_stephenson_s()` function will repeatedly generate datasets and analyze them, and then return a summary of the results. We focus on the top 35 quantiles.

```{r secret_simulation_block, include=FALSE}
if ( !file.exists( here::here( "demo_code/heller_s_check_results.rds" ) ) ) {
    checks$data = pmap( checks, function( TxVar, tx_dist, rho ) {
        cat( glue::glue("Running {TxVar} {tx_dist} rho={rho}\n" ) )
        explore_stephenson_s( s = s_list,
                              n = nrow( electric_teachers ),
                              Y0_distribution = cdat$std_gain,
                              tx_function = tx_dist,
                              nperm = nperm,
                              ATE = ate, tx_scale=TxVar, rho = rho,
                              R = R, calc_ICC = FALSE,
                              parallel = TRUE, n_workers = 5,
                              verbose = TRUE,
                              targeted_power = FALSE, k.vec = (233-100):233 )
    } )
    s_selector = unnest( checks, cols = "data" )
    
    saveRDS( s_selector, here::here( "demo_code/heller_s_check_results.rds" ) )
} else {
    s_selector = readRDS( here::here( "demo_code/heller_s_check_results.rds" ) )
}
```

```{r demo_simulation_code_block, eval=FALSE}
checks$data = pmap( checks, function( TxVar, tx_dist, rho ) {
    cat( glue::glue("Running {TxVar} {tx_dist} rho={rho}\n" ) )
    explore_stephenson_s( s = s_list,
                          n = nrow( dat ),
                          Y0_distribution = cdat$std_gain,
                          tx_function = tx_dist,
                          ATE = ate, tx_scale=TxVar, rho = rho,
                          nperm = nperm, R = R, 
                          calc_ICC = FALSE, parallel = TRUE,
                          targeted_power = FALSE, k.vec = (233-100):233 )
} )
checks
s_selector = unnest( checks, cols = "data" )
```

We note for several scenarios and quantiles, the median lower bound to the CI is $-\infty$.
To allow for aggregation, we impute a lower bound for these quantiles of -3.79, based on our calculation above:
```{r}
s_selector$q_ci[ !is.finite( s_selector$q_ci ) ] = -3.79
```


We can then aggregate our findings and see what $s$ provides the tightest confidence intervals across a range of quantile groups:
```{r plot_s_selector, warning=FALSE}
n_units = nrow(electric_teachers)
s_selector = mutate( s_selector,
                     group = cut( k, 
                                  breaks = c(0, 170, 200, 220, 233),
                                  labels = c( "v low", "low", "med", "high" ) ) )

s_selector$rho.f = factor( s_selector$rho,
                          levels = c( -0.5, 0, 0.5 ),
                          labels = c( "rho = -0.5", "rho = 0", "rho = 0.5" ) )

avg = s_selector %>% group_by( s, group, rho.f ) %>%
    summarise( q_ci = median( q_ci ), .groups="drop" ) %>%
    filter( group != "v low", is.finite( q_ci ) )

s_selector %>% filter( group != "v low" ) %>%
    ggplot( aes( s, q_ci ) ) +
    facet_grid( rho.f ~ group ) +
    geom_hline( yintercept = 0 ) +
    geom_smooth( aes( col=tx_dist ), method = "loess", se = FALSE, 
                 span = 1, lwd= 0.5 ) +
    labs( x = "s", y = "Median lower CI bound", col = "tx distribution:" ) +
    scale_x_log10( breaks = unique( s_selector$s ) ) +
    theme_minimal() +
    theme( panel.grid.minor = element_blank() ) + 
    geom_point( data = avg, col="black", size=2 ) +
    coord_cartesian( ylim = c( -1, 2 ) ) +
    theme( legend.position="bottom", 
                                legend.direction="horizontal", legend.key.width=unit(1,"cm"),
           panel.border = element_rect(colour = "grey", fill=NA, size=1) )
    
```

```{r include=FALSE, eval=FALSE}
ggsave( filename = "demo_code/s_selector.pdf", width = 8, height = 4 )
```



We identify which $s$ gave the highest power across the different groups as follows:
```{r, warning=FALSE}
avg %>% ungroup() %>%
    group_by( rho.f, group ) %>%
    filter( q_ci == max(q_ci) ) %>%
    dplyr::select(-q_ci) %>%
    pivot_wider( names_from="rho.f", values_from="s" ) %>%
    knitr::kable()
```

We can explore which $s$ gives the highest bound on the number of units positively impacted by treatment:
```{r}
s_num_n <- s_selector %>% 
    group_by( TxVar, tx_dist, rho, group ) %>%
    filter( q_ci == max(q_ci) )
table( s_num_n$group, s_num_n$s )
```
    
Across these results, $s = 6$ looks like a reasonable choice.
If we believe the correlation of $Y_i(0)$ and $\tau_i$ is negative, we might tend to a smaller $s$.
Given the apparent shift of the treatment vs. control distributions in our data, either treatment variation is slight or there is a negative correlation between $Y_i(0)$ and $\tau_i$ (otherwise the variance of the treated group would be larger than the control group, and they are instead quite similar).
For number of units tagged as significant, we see $s=2$ through $6$, primarily, across scenarios.
We thus might adjust our $s$ down a bit to try and optimize multiple goals at once, using, e.g., $s = 5$.


## Analyzing the data

With our selected $s$ in hand, we first test the bounded null hypothesis that all individual effects are non-positive. 
```{r, cache=TRUE}
nperm = 10^4
pval.steph5 <- pval_quantile(Z=electric_teachers$TxAny, Y=electric_teachers$gain, 
                              k=n_units, c=0, alternative = "greater", 
                              method.list=list(name="Stephenson", s=5), nperm=nperm )
pval.steph5 # final p-value
```


We then conduct inference for all quantiles of individual treatment effects. The following figures show the 90% lower confidence limits for all quantiles of individual effects using Stephenson rank statistics with s = 5 and s = 2 (i.e., Wilcoxon rank), respectively. 

```{r, fig.height = 4, fig.width = 6.4, fig.align = "center"}
ci5 = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, alternative="greater", 
                  method.list=list( name="Stephenson", s=5 ), nperm=nperm, alpha=0.10 )
ci2 = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, alternative="greater", 
                  method.list=list( name="Stephenson", s=2 ), nperm=nperm, alpha=0.10 )
par(mfrow = c(1, 2))
plot_quantile_CIs(ci5, k_start = 102, main = "s=5" )
plot_quantile_CIs(ci2, k_start = 102, main = "s=2" )
```

Finally, we infer the effect range by conducting two tests, one for greater and one for less.  We then can say that there are effects at least as small as some value and at least as large as another.
```{r, cache=TRUE}
tau.max.lower = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, k.vec=n_units, alternative="greater", 
              method.list=list( name="Stephenson", s=5 ), nperm=nperm, alpha=0.05 )$lower
tau.min.upper = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, k.vec=1, alternative="less", 
              method.list=list( name="Stephenson", s=5 ), nperm=nperm, alpha=0.05 )$upper
max(0, tau.max.lower - tau.min.upper)
```

In this case, we have no evidence of effect heterogeniety (all the effects could be the same).

```{r, include=FALSE}
save.image("Develop20220921.RData") # save the analysis result
```



# Evaluating the effects of six-month nutrition therapy

We get the data for the homefood study from <https://doi.org/10.7910/DVN/38X3LX>.
We download the SPSS Binary (Original File Format) there. The details of the study can be found at <https://clinicaltrials.gov/ct2/show/NCT03995303>.
Once we have the file, we can load as follows:

```{r, warning=FALSE}
dat = foreign::read.spss(here::here( "demo_code/Data file, 24.09.2021.sav" ),
                         to.data.frame=TRUE)
```

We focus on the effect of the treatment on the increase of lean body mass (kg) measured before and after the trial. We exclude two units with missing outcomes, resulting in 52 treated units and 52 control units.
```{r}
dat$change = dat$lean_mass_kg_1 - dat$lean_mass_kg_0
paste("There are", sum(is.na(dat$change)), "units with missing outcomes.")
dat = dat[ !is.na(dat$change), ] # delete units with missing outcomes
n = nrow(dat)
dat = dat[sample(n), ] # randomly permute the order of units
table(dat$group)
```

The following figure shows the histograms of the lean body mass changes in treated and control groups, respectively.
```{r, fig.height = 4, fig.width = 3.2, fig.align = "center"}
hist(dat$change[dat$group=="intervention"], xlim=range(dat$change), freq=FALSE, 
     ylim=c(0,0.20), col="grey", border=FALSE, breaks = 10, main=NULL, xlab="gain score")
hist(dat$change[dat$group=="control"], freq=FALSE, add=TRUE, breaks=10, col=NULL)
```

We infer the lower confidence limits for all quantiles of individual treatment effects. The following two figures show the 90% lower confidence limits for all quantiles of individual effects using Stephenson rank statistics with s = 6 and s = 2 (i.e., Wilcoxon rank), respectively
```{r, fig.height = 4, fig.width = 6.4, fig.align = "center", cache=TRUE}
nperm = 10^5
ci6 = ci_quantile( Z=as.numeric(dat$group == "intervention"), Y=dat$change, 
                   alternative="greater", method.list=list(name="Stephenson", s=6), 
                   nperm=nperm, alpha=0.10 )
ci2 = ci_quantile( Z=as.numeric(dat$group == "intervention"), Y=dat$change, 
                   alternative="greater", method.list=list(name="Stephenson", s=2), 
                   nperm=nperm, alpha=0.10 )

par(mfrow = c(1, 2))
plot_quantile_CIs(ci6, main="s=6", k_start = n-37)
plot_quantile_CIs(ci2, main="s=2", k_start = n-37)
```

```{r, include=FALSE}
save.image("Homefood20220921.RData") # save the analysis result
```


