---
title: "Data analysis and power analysis using the developed package RIQITE"
author: "Devin Caughey, Allan Dafoe, Xinran Li, and Luke Miratrix"
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

---
bibliography: ["permutation.bib"]
biblio-style: "apalike"
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Dropbox/Own/Projects/Quantile Effects/code2021Aug/R illustration")
library(dplyr)
library(ggplot2)
library( RIQITE )
set.seed(1)
nperm = 1000
```

# Install the RIQITE package
We first install the package from Github. 
The github page of the package (<https://github.com/li-xinran/RIQITE>) contains main functions with detailed explanation in R documentation, as well as a simple illustrating example. 
Below we provide the code used to analyze the data sets in the paper. 

Install via `devtools`, and then load:
```{r, message=FALSE, eval=FALSE}
devtools::install_github("li-xinran/RIQITE")
library(RIQITE)
```


# Testing monotonicity of an instrumental variable

We get the data from @BlackEtAl11a, in which Table 1 cross-tabulates month of birth and school entry age (early/on-time/late) in terms of proportions and Table 3 reports the total number of subjects born in January or December (the "discontinuity subsample"): 104,023. We round the total number of subjects to 104,000, and, similar to @FioriniStevens14a, we assume there are equal numbers of subjects born in January and December.
Based on these summary statistics, we generate the individual-level data as follows:

```{r get the data, message=FALSE}
x <- matrix(0, 2, 3, dimnames=list(Month=c("Dec", "Jan"), 
                                   Entry=c("Early", "On-Time", "Late")))
n <- 104000
x["Dec", "On-Time"] <- .85
x["Dec", "Late"] <- .15
x["Jan", "Early"] <- .10
x["Jan", "On-Time"] <- .90
df <- data.frame(BirthMonth = rep(c("Dec", "Jan"), each=n/2), 
                 EntryTiming = c(rep("On-Time", n/2 * .85), rep("Late", n/2 * .15), 
                                 rep("Early", n/2 * .10), rep("On-Time", n/2 * .90)))
df <- df %>% mutate(EntryAge = NA,
        EntryAge = ifelse(BirthMonth == "Dec" & EntryTiming == "On-Time", 6.75, EntryAge),
        EntryAge = ifelse(BirthMonth == "Dec" & EntryTiming == "Late", 7.75, EntryAge),
        EntryAge = ifelse(BirthMonth == "Jan" & EntryTiming == "On-Time", 23/3, EntryAge),
        EntryAge = ifelse(BirthMonth == "Jan" & EntryTiming == "Early", 20/3, EntryAge))
data <- df[sample(c(1:n)), ] # randomly permute the ordering of units
table(data$BirthMonth, data$EntryTiming)
```

The following figure shows the empirical distribution functions of the school-entry age of December-born and January-born subjects. From the figure, most children started school at an older age if they were born in January rather than December.

```{r, fig.height = 3, fig.width = 3, fig.align = "center"}
ggplot(data, aes(EntryAge, color = BirthMonth, linetype=BirthMonth)) +
  stat_ecdf(geom="step", show.legend=FALSE, size=1.3) +
  annotate("text", x=c(7.2, 7.2), y=c(.145, .81), 
           label=c("January", "December")) +
  scale_x_continuous(breaks=seq(6, 8, .2)) + 
  theme_bw() + 
  theme( plot.background = element_blank(), 
         panel.grid.major = element_blank(), 
         panel.grid.minor = element_blank(),
         plot.title=element_text(hjust=.5 ) ) + 
  labs(x="School-Entry Age (Years)", y="Probability") 
```

We first perform the standard regression analysis assessing the relation between the instrument (birthday) and the school-entry age. The first-stage relationship is incredibly strong, with an average effect of $-0.667$ years and an F statistic of 106,256. This would conventionally be considered persuasive evidence of a valid instrument.

```{r F test}
lm.fit <- lm(EntryAge ~ (BirthMonth=="Dec"), data)
cat(paste( 
  "esimated average effect is", round( as.numeric(lm.fit$coefficients[2]), digits = 3), 
  "and F statistic is", round( as.numeric( summary(lm.fit)$fstatistic[1] ), digits = 0), "\n" 
  ))
```

We then perform the randomization test for the bounded null $H_{\le 0}$ on the monotonicity of the instrument. Specifically, we consider three randomization tests using the difference-in-means statistic, Wilcoxon rank sum statistic and Stephenson rank sum statistic with the parameter $s=10$, respectively. We also conduct the classical Student's t-test. These produce Table 1 in the paper.

```{r p value for bounded null}
data$Z = as.numeric( data$BirthMonth == "Dec" )
data$Y = data$EntryAge
n <- length(data$Z)
m <- sum(data$Z)
nperm <- 10^4 # number of permutations for Monte Carlo approximation

# Generate a sample of permutations
Z.perm <- assign_CRE(n, m, nperm)

# difference-in-means
pval.dim <- pval_bound(Z=data$Z, Y=data$Y, c=0, method.list=list(name = "DIM"), 
                     alternative = "greater", Z.perm=Z.perm, impute="control")
ci.dim<-ci_bound(Z=data$Z, Y=data$Y, alternative="greater", method.list=list(name="DIM"), 
                        Z.perm=Z.perm, impute="control", alpha=0.10)

# Wilcoxon rank sum
pval.wilc <- pval_quantile(Z=data$Z, Y=data$Y, k=n, c=0, alternative = "greater",
                           method.list=list(name="Stephenson", s=2), Z.perm=Z.perm )
ci.wilc <- ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
                method.list=list(name="Stephenson", s=2), Z.perm=Z.perm,  alpha=0.10)

# Stephenson rank sum
pval.steph10 <- pval_quantile(Z=data$Z, Y=data$Y, k=n, c=0, alternative = "greater", 
                              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm )
ci.steph10 <- ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.10)

# classical Student's t-test
ttest = t.test(data$Y[data$Z==1], data$Y[data$Z==0], alternative = "greater")
pval.ttest = ttest$p.value
ci.ttest = round( ttest$conf.int[1], digits = 3)

# result for Table 1 in the paper
result = data.frame(test=c("diff-in-means", "Wilcoxon", "Stephenson 10", "t test"),
                    pval=c(pval.dim, pval.wilc, pval.steph10, pval.ttest), 
                    lower = c(ci.dim, ci.wilc$lower, ci.steph10$lower, ci.ttest))
knitr::kable( result )
```

Finally, we infer the effect range. The result shows that the range of the effect of birth month on school-entry age is at least 1 year, indicating significant individual effect heterogeneity. 
```{r}
tau.max.lower = ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.05)$lower
tau.min.upper = ci_quantile(Z=data$Z, Y=data$Y, k.vec = 1, alternative = "less", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.05)$upper
print(paste( "lower confidence limit for effect range is", 
             max(0, tau.max.lower - tau.min.upper) ))
save.image("BirthSchoolAge20220921.RData") # save the analysis result
```



# Evaluating the effectiveness of professional development

We get the data from @HSMHSD10.
The cleaned data, as described in the paper, is part of the package.
We load it and estimate ATE via linear regression:

```{r get the professional development data, message=FALSE}
data( electric_teachers )

mod <- lm(gain~TxAny, data = electric_teachers)
round(mod$coefficients[2], digits = 2)
```

The following figure shows the histograms of the observed gain scores in the treatment and control groups. 
```{r, fig.height = 4, fig.width = 3.2, fig.align = "center"}
hist(electric_teachers$gain[electric_teachers$TxAny==1], 
     xlim = range(electric_teachers$gain), freq = FALSE, ylim = c(0, 0.065), 
     col = "grey", border=FALSE, breaks = 20,
     main = NULL, xlab = "gain score")
hist(electric_teachers$gain[electric_teachers$TxAny==0], 
     freq = FALSE, add = TRUE, breaks = 20, col = NULL)
```

We first test the bounded null hypothesis that all individual effects are non-positive. 
```{r}
nperm = 10^4
pval.steph10 <- pval_quantile(Z=electric_teachers$TxAny, Y=electric_teachers$gain, 
                              k=n, c=0, alternative = "greater", 
                              method.list=list(name="Stephenson", s=10), nperm=nperm )
pval.steph10 # final p-value
```



We then conduct inference for all quantiles of individual treatment effects. The following figures show the 90% lower confidence limits for all quantiles of individual effects using Stephenson rank statistics with s = 10 and s = 2 (i.e., Wilcoxon rank), respectively. 
```{r, fig.height = 4, fig.width = 6.4, fig.align = "center"}
ci10 = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, alternative="greater", 
                  method.list=list( name="Stephenson", s=10 ), nperm=nperm, alpha=0.10 )
ci2 = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, alternative="greater", 
                  method.list=list( name="Stephenson", s=2 ), nperm=nperm, alpha=0.10 )
par(mfrow = c(1, 2))
plot_quantile_CIs(ci10, k_start = 102, main = "s=10" )
plot_quantile_CIs(ci2, k_start = 102, main = "s=2" )
```

Finally, we infer the effect range.
```{r}
tau.max.lower = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, k.vec=n, alternative="greater", 
              method.list=list( name="Stephenson", s=10 ), nperm=nperm, alpha=0.05 )$lower
tau.min.upper = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, k.vec=1, alternative="less", 
              method.list=list( name="Stephenson", s=10 ), nperm=nperm, alpha=0.05 )$upper
max(0, tau.max.lower - tau.min.upper)
```


```{r, include=FALSE}
save.image("Develop20220921.RData") # save the analysis result
```



# Evaluating the effects of six-month nutrition therapy

We get the data for the homefood study from <https://doi.org/10.7910/DVN/38X3LX>.
We download the SPSS Binary (Original File Format) there. The details of the study can be found at <https://clinicaltrials.gov/ct2/show/NCT03995303>.
Once we have the file, we can load as follows:

```{r, warning=FALSE}
dat = foreign::read.spss(here::here( "demo_code/Data file, 24.09.2021.sav" ),
                         to.data.frame=TRUE)
```

We focus on the effect of the treatment on the increase of lean body mass (kg) measured before and after the trial. We exclude two units with missing outcomes, resulting in 52 treated units and 52 control units.
```{r}
dat$change = dat$lean_mass_kg_1 - dat$lean_mass_kg_0
paste("There are", sum(is.na(dat$change)), "units with missing outcomes.")
dat = dat[ !is.na(dat$change), ] # delete units with missing outcomes
n = nrow(dat)
dat = dat[sample(n), ] # randomly permute the order of units
table(dat$group)
```

The following figure shows the histograms of the lean body mass changes in treated and control groups, respectively.
```{r, fig.height = 4, fig.width = 3.2, fig.align = "center"}
hist(dat$change[dat$group=="intervention"], xlim=range(dat$change), freq=FALSE, 
     ylim=c(0,0.20), col="grey", border=FALSE, breaks = 10, main=NULL, xlab="gain score")
hist(dat$change[dat$group=="control"], freq=FALSE, add=TRUE, breaks=10, col=NULL)
```

We infer the lower confidence limits for all quantiles of individual treatment effects. The following two figures show the 90% lower confidence limits for all quantiles of individual effects using Stephenson rank statistics with s = 6 and s = 2 (i.e., Wilcoxon rank), respectively
```{r, fig.height = 4, fig.width = 6.4, fig.align = "center"}
nperm = 10^5
ci6 = ci_quantile( Z=as.numeric(dat$group == "intervention"), Y=dat$change, 
                   alternative="greater", method.list=list(name="Stephenson", s=6), 
                   nperm=nperm, alpha=0.10 )
ci2 = ci_quantile( Z=as.numeric(dat$group == "intervention"), Y=dat$change, 
                   alternative="greater", method.list=list(name="Stephenson", s=2), 
                   nperm=nperm, alpha=0.10 )
par(mfrow = c(1, 2))
plot_quantile_CIs(ci6, main="s=6", k_start = n-37)
plot_quantile_CIs(ci2, main="s=2", k_start = n-37)
```

```{r, include=FALSE}
save.image("Homefood20220921.RData") # save the analysis result
```



# Power comparison among different Stephenson rank sum statistics

We first give the data generating process. Here we consider the case where the control potential outcome follows the standard Gaussian distribution and the treatment potential outcome follows the Gaussian distribution with mean 1 and variance 2. This include the case where the individual effect follows Gaussian distribution mean 1 and variance 1, independent of the control potential outcome.  
```{r}
rm(list=ls())
library(RIQITE)
gen_PO <- function(n){
  mu0 = 0
  sd0 = 1
  mu1 = 1
  sd1 = 2
  Y1 = rnorm(n, mean = mu1, sd = sd1)
  Y0 = rnorm(n, mean = mu0, sd = sd0)
  return(data.frame(Y1 = Y1, Y0 = Y0))
}
```

We then conduct simulation over 100 simulated data sets, using Stephenson rank sum statistics with parameter $s$ equal to 2, 6 and 10. 
```{r}
n = 120
iter.max = 500
nperm = 10^5
simu.result <- simu_power( gen=gen_PO, n=n, treat.prop=0.5, iter.max=iter.max, 
                           Steph.s.vec=c(2, 6, 10), alternative="greater", alpha=0.1, 
                           nperm=nperm, print.iter=100 )
save.image("PowerSimu20220921.RData") # save the simulation result
```

The code below compares the average lower confidence limits of $n(0)$, the number of units with effects greater than 0.
```{r}
summary_power( result=simu.result, measure="CI_n(c)", alternative="greater", cutoff=0  )
```

The code below compares the average lower confidence limits of $tau_{(k)}$, the individual effect at rank $k$. 
```{r}
summary_power( result=simu.result, measure="CI_tau(k)", alternative="greater", 
               cutoff=NULL, k=n  )
```

The code below compares the power for testing the null hypothesis $H_0: \tau_{(k)} \le 1$, with $k$ equals the sample size $n$.
```{r}
summary_power( result=simu.result, measure="test_tau(k)", alternative="greater", 
               cutoff=1, k=n  )
```

