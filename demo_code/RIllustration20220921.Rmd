---
title: "Data analysis and power analysis using the developed package RIQITE"
author: "Devin Caughey, Allan Dafoe, Xinran Li, and Luke Miratrix"
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

---
bibliography: ["permutation.bib"]
biblio-style: "apalike"
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Dropbox/Own/Projects/Quantile Effects/code2021Aug/R illustration")
library(dplyr)
library(ggplot2)
library( RIQITE )
set.seed(1)
nperm = 1000
```

# Install the RIQITE package
We first install the package from Github. 
The github page of the package (<https://github.com/li-xinran/RIQITE>) contains main functions with detailed explanation in R documentation, as well as a simple illustrating example. 
Below we provide the code used to analyze the data sets in the paper. 

Install via `devtools`, and then load:
```{r, message=FALSE, eval=FALSE}
devtools::install_github("li-xinran/RIQITE")
library(RIQITE)
```


# Testing monotonicity of an instrumental variable

We get the data from @BlackEtAl11a, in which Table 1 cross-tabulates month of birth and school entry age (early/on-time/late) in terms of proportions and Table 3 reports the total number of subjects born in January or December (the "discontinuity subsample"): 104,023. We round the total number of subjects to 104,000, and, similar to @FioriniStevens14a, we assume there are equal numbers of subjects born in January and December.
Based on these summary statistics, we generate the individual-level data as follows:

```{r get the data, message=FALSE}
x <- matrix(0, 2, 3, dimnames=list(Month=c("Dec", "Jan"), 
                                   Entry=c("Early", "On-Time", "Late")))
n <- 104000
x["Dec", "On-Time"] <- .85
x["Dec", "Late"] <- .15
x["Jan", "Early"] <- .10
x["Jan", "On-Time"] <- .90
df <- data.frame(BirthMonth = rep(c("Dec", "Jan"), each=n/2), 
                 EntryTiming = c(rep("On-Time", n/2 * .85), rep("Late", n/2 * .15), 
                                 rep("Early", n/2 * .10), rep("On-Time", n/2 * .90)))
df <- df %>% mutate(EntryAge = NA,
        EntryAge = ifelse(BirthMonth == "Dec" & EntryTiming == "On-Time", 6.75, EntryAge),
        EntryAge = ifelse(BirthMonth == "Dec" & EntryTiming == "Late", 7.75, EntryAge),
        EntryAge = ifelse(BirthMonth == "Jan" & EntryTiming == "On-Time", 23/3, EntryAge),
        EntryAge = ifelse(BirthMonth == "Jan" & EntryTiming == "Early", 20/3, EntryAge))
data <- df[sample(c(1:n)), ] # randomly permute the ordering of units
table(data$BirthMonth, data$EntryTiming)
```

The following figure shows the empirical distribution functions of the school-entry age of December-born and January-born subjects. From the figure, most children started school at an older age if they were born in January rather than December.

```{r, fig.height = 3, fig.width = 3, fig.align = "center"}
ggplot(data, aes(EntryAge, color = BirthMonth, linetype=BirthMonth)) +
  stat_ecdf(geom="step", show.legend=FALSE, size=1.3) +
  annotate("text", x=c(7.2, 7.2), y=c(.145, .81), 
           label=c("January", "December")) +
  scale_x_continuous(breaks=seq(6, 8, .2)) + 
  theme_bw() + 
  theme( plot.background = element_blank(), 
         panel.grid.major = element_blank(), 
         panel.grid.minor = element_blank(),
         plot.title=element_text(hjust=.5 ) ) + 
  labs(x="School-Entry Age (Years)", y="Probability") 
```

We first perform the standard regression analysis assessing the relation between the instrument (birthday) and the school-entry age. The first-stage relationship is incredibly strong, with an average effect of $-0.667$ years and an F statistic of 106,256. This would conventionally be considered persuasive evidence of a valid instrument.

```{r F test}
lm.fit <- lm(EntryAge ~ (BirthMonth=="Dec"), data)
cat(paste( 
  "esimated average effect is", round( as.numeric(lm.fit$coefficients[2]), digits = 3), 
  "and F statistic is", round( as.numeric( summary(lm.fit)$fstatistic[1] ), digits = 0), "\n" 
  ))
```

We then perform the randomization test for the bounded null $H_{\le 0}$ on the monotonicity of the instrument. Specifically, we consider three randomization tests using the difference-in-means statistic, Wilcoxon rank sum statistic and Stephenson rank sum statistic with the parameter $s=10$, respectively. We also conduct the classical Student's $t$-test. These produce Table 1 in the paper.

```{r p value for bounded null, cache=TRUE}
data$Z = as.numeric( data$BirthMonth == "Dec" )
data$Y = data$EntryAge
n <- length(data$Z)
m <- sum(data$Z)
nperm <- 10^4 # number of permutations for Monte Carlo approximation

# Generate a sample of permutations
Z.perm <- assign_CRE(n, m, nperm)

# difference-in-means
pval.dim <- pval_bound(Z=data$Z, Y=data$Y, c=0, method.list=list(name = "DIM"), 
                     alternative = "greater", Z.perm=Z.perm, impute="control")
ci.dim <- ci_bound(Z=data$Z, Y=data$Y, alternative="greater", method.list=list(name="DIM"), 
                        Z.perm=Z.perm, impute="control", alpha=0.10)

# Wilcoxon rank sum
pval.wilc <- pval_quantile(Z=data$Z, Y=data$Y, k=n, c=0, alternative = "greater",
                           method.list=list(name="Stephenson", s=2), Z.perm=Z.perm )
ci.wilc <- ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
                method.list=list(name="Stephenson", s=2), Z.perm=Z.perm,  alpha=0.10)

# Stephenson rank sum
pval.steph10 <- pval_quantile(Z=data$Z, Y=data$Y, k=n, c=0, alternative = "greater", 
                              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm )
ci.steph10 <- ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.10)

# classical Student's t-test
ttest = t.test(data$Y[data$Z==1], data$Y[data$Z==0], alternative = "greater")
pval.ttest = ttest$p.value
ci.ttest = round( ttest$conf.int[1], digits = 3)

# result for Table 1 in the paper
result = data.frame(test=c("diff-in-means", "Wilcoxon", "Stephenson 10", "t test"),
                    pval=c(pval.dim, pval.wilc, pval.steph10, pval.ttest), 
                    lower = c(ci.dim, ci.wilc$lower, ci.steph10$lower, ci.ttest))
knitr::kable( result )
```

Finally, we infer the effect range. The result shows that the range of the effect of birth month on school-entry age is at least 1 year, indicating significant individual effect heterogeneity. 
```{r}
tau.max.lower = ci_quantile(Z=data$Z, Y=data$Y, k.vec = n, alternative = "greater", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.05)$lower
tau.min.upper = ci_quantile(Z=data$Z, Y=data$Y, k.vec = 1, alternative = "less", 
              method.list=list(name="Stephenson", s=10), Z.perm=Z.perm,  alpha=0.05)$upper
print(paste( "lower confidence limit for effect range is", 
             max(0, tau.max.lower - tau.min.upper) ))
save.image("BirthSchoolAge20220921.RData") # save the analysis result
```



# Evaluating the effectiveness of professional development

We get the data from @HSMHSD10.
The cleaned data, as described in the paper, is part of the package.
We load it and estimate ATE via linear regression:

```{r get the professional development data, message=FALSE}
data( electric_teachers )

mod <- lm(gain~TxAny, data = electric_teachers)
round(mod$coefficients[2], digits = 2)
```

The following figure shows the histograms of the observed gain scores in the treatment and control groups. 
```{r, fig.height = 4, fig.width = 3.2, fig.align = "center"}
hist(electric_teachers$gain[electric_teachers$TxAny==1], 
     xlim = range(electric_teachers$gain), freq = FALSE, ylim = c(0, 0.065), 
     col = "grey", border=FALSE, breaks = 20,
     main = NULL, xlab = "gain score")
hist(electric_teachers$gain[electric_teachers$TxAny==0], 
     freq = FALSE, add = TRUE, breaks = 20, col = NULL)
```


## Determining an s-value for the test statistic

We plan on using a Stephenson's Rank Sum test, but need to determine an appropriate value of $s$.
We first conduct a sensitivity analysis across a range of scenarios to determine which $s$ have the highest amount of power for the  top 5 quantiles, the next 10 quantiles, and the next 15 quantiles.

We first get the empirical distribution of the control outcomes, and make a grid of all scenarios to explore:
```{r}
cdat = filter( dat, TxAny == 0 ) # Get standardized outcome
EstTx = 0.65
TxVar = 0.5
R = 100
s_list = c( 2, 3, 4, 5, 6, 8, 10, 15, 20 )

checks = expand_grid( TxVar = c( 0.5, 1.0 ),
                      tx_dist = c( "constant", "rexp", "rnorm" ),
                      rho = c( -0.5, 0, 0.5 ) )
checks = filter( checks, rho == 0 | tx_dist != "constant" )
```

This gives `r nrow(checks)` scenarios. We use `pmap()` to run `explore_stephenson_s` across them.
The `explore_stephenson_s()` function will repeatedly generate datasets and analyze them, and then return a summary of the results. We focus on the top 35 quantiles.

```{r secret_simulation_block, include=FALSE}
if ( !file.exists( here::here( "demo_code/heller_s_check_results.rds" ) ) ) {
    checks$data = pmap( checks, function( TxVar, tx_dist, rho ) {
    cat( glue::glue("Running {TxVar} {tx_dist} {rho}\n" ) )
    explore_stephenson_s( s = s_list,
                          n = nrow( dat ),
                          Y0_distribution = cdat$std_gain,
                          tx_function = tx_function_factory(tx_dist,
                                                            ATE = EstTx, tx_scale=TxVar, rho = rho ),
                          R = R, calc_ICC = TRUE, parallel = TRUE,
                          targeted_power = FALSE, k.vec = (233-35):233 )
} )
    s_selector = unnest( checks, cols = "data" )

saveRDS( s_selector, here::here( "demo_code/heller_s_check_results.rds" ) )
} else {
    s_selector = readRDS( here::here( "demo_code/heller_s_check_results.rds" ) )

}
```

```{r demo_simulation_code_block, eval=FALSE}
checks$data = pmap( checks, function( TxVar, tx_dist, rho ) {
    cat( glue::glue("Running {TxVar} {tx_dist} {rho}\n" ) )
    explore_stephenson_s( s = s_list,
                          n = nrow( dat ),
                          Y0_distribution = cdat$std_gain,
                          tx_function = tx_function_factory(tx_dist,
                                                            ATE = EstTx, tx_scale=TxVar, rho = rho ),
                          R = R, calc_ICC = TRUE, parallel = TRUE,
                          targeted_power = FALSE, k.vec = (233-35):233 )
} )
checks
s_selector = unnest( checks, cols = "data" )
```

We can then aggregate our findings and see what $s$ provides greatest power across a range of quantile groups:
```{r}
n_units = nrow(dat)
s_selector = mutate( s_selector,
                     group = ifelse( k >= n_units - 5, "high",
                                     ifelse( k >= n_units - 15, "med", "low" ) ) ) %>%
    mutate( group = factor( group, levels = c( "high", "med", "low" ) ) )

avg = s_selector %>% group_by( s, group ) %>%
    summarise( power = mean( power ), .groups="drop" )
avg

ggplot( s_selector, aes( s, power ) ) +
    facet_wrap( ~ group, nrow = 1 ) +
    geom_hline( yintercept = 0.80, lty = 2 ) +
    geom_smooth( aes( col=tx_dist ), method = "loess", se = FALSE, 
                 span = 1, lwd= 0.5 ) +
    labs( x = "s", y = "Power" ) +
    scale_x_log10( breaks = unique( s_selector$s ) ) +
    coord_cartesian( ylim=c(0,1) ) +
    theme_minimal() + 
    geom_point( data = avg, col="black", size=2 )
```

We identify best $s$ as follows:
```{r}
avg %>% ungroup() %>%
    group_by( group ) %>%
    filter( power == max(power) )
```

We can also select $s$ based on which $s$ will give the most units, in general, with significant intervals:
```{r}
s_num_n <- s_selector %>% 
    group_by( s, TxVar, tx_dist ) %>%
    summarise( n = mean( n ) )

s_num_n %>% group_by( TxVar, tx_dist ) %>%
    filter( n == max(n) )
```
    
Across these results, $s = 5$ looks like a reasonable choice.


## Analyzing the data

With our selected $s$ in hand, we first test the bounded null hypothesis that all individual effects are non-positive. 
```{r, cache=TRUE}
nperm = 10^4
pval.steph5 <- pval_quantile(Z=electric_teachers$TxAny, Y=electric_teachers$gain, 
                              k=n, c=0, alternative = "greater", 
                              method.list=list(name="Stephenson", s=5), nperm=nperm )
pval.steph5 # final p-value
```


We then conduct inference for all quantiles of individual treatment effects. The following figures show the 90% lower confidence limits for all quantiles of individual effects using Stephenson rank statistics with s = 5 and s = 2 (i.e., Wilcoxon rank), respectively. 

```{r, fig.height = 4, fig.width = 6.4, fig.align = "center"}
ci5 = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, alternative="greater", 
                  method.list=list( name="Stephenson", s=5 ), nperm=nperm, alpha=0.10 )
ci2 = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, alternative="greater", 
                  method.list=list( name="Stephenson", s=2 ), nperm=nperm, alpha=0.10 )
par(mfrow = c(1, 2))
plot_quantile_CIs(ci5, k_start = 102, main = "s=5" )
plot_quantile_CIs(ci2, k_start = 102, main = "s=2" )
```

Finally, we infer the effect range by conducting two tests, one for greater and one for less.  We then can say that there are effects at least as small as some value and at least as large as another.
```{r, cache=TRUE}
tau.max.lower = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, k.vec=n, alternative="greater", 
              method.list=list( name="Stephenson", s=5 ), nperm=nperm, alpha=0.05 )$lower
tau.min.upper = ci_quantile( Z=electric_teachers$TxAny, Y=electric_teachers$gain, k.vec=1, alternative="less", 
              method.list=list( name="Stephenson", s=5 ), nperm=nperm, alpha=0.05 )$upper
max(0, tau.max.lower - tau.min.upper)
```

In this case, we have no evidence of effect heterogeniety (all the effects could be the same).

```{r, include=FALSE}
save.image("Develop20220921.RData") # save the analysis result
```



# Evaluating the effects of six-month nutrition therapy

We get the data for the homefood study from <https://doi.org/10.7910/DVN/38X3LX>.
We download the SPSS Binary (Original File Format) there. The details of the study can be found at <https://clinicaltrials.gov/ct2/show/NCT03995303>.
Once we have the file, we can load as follows:

```{r, warning=FALSE}
dat = foreign::read.spss(here::here( "demo_code/Data file, 24.09.2021.sav" ),
                         to.data.frame=TRUE)
```

We focus on the effect of the treatment on the increase of lean body mass (kg) measured before and after the trial. We exclude two units with missing outcomes, resulting in 52 treated units and 52 control units.
```{r}
dat$change = dat$lean_mass_kg_1 - dat$lean_mass_kg_0
paste("There are", sum(is.na(dat$change)), "units with missing outcomes.")
dat = dat[ !is.na(dat$change), ] # delete units with missing outcomes
n = nrow(dat)
dat = dat[sample(n), ] # randomly permute the order of units
table(dat$group)
```

The following figure shows the histograms of the lean body mass changes in treated and control groups, respectively.
```{r, fig.height = 4, fig.width = 3.2, fig.align = "center"}
hist(dat$change[dat$group=="intervention"], xlim=range(dat$change), freq=FALSE, 
     ylim=c(0,0.20), col="grey", border=FALSE, breaks = 10, main=NULL, xlab="gain score")
hist(dat$change[dat$group=="control"], freq=FALSE, add=TRUE, breaks=10, col=NULL)
```

We infer the lower confidence limits for all quantiles of individual treatment effects. The following two figures show the 90% lower confidence limits for all quantiles of individual effects using Stephenson rank statistics with s = 6 and s = 2 (i.e., Wilcoxon rank), respectively
```{r, fig.height = 4, fig.width = 6.4, fig.align = "center", cache=TRUE}
nperm = 10^5
ci6 = ci_quantile( Z=as.numeric(dat$group == "intervention"), Y=dat$change, 
                   alternative="greater", method.list=list(name="Stephenson", s=6), 
                   nperm=nperm, alpha=0.10 )
ci2 = ci_quantile( Z=as.numeric(dat$group == "intervention"), Y=dat$change, 
                   alternative="greater", method.list=list(name="Stephenson", s=2), 
                   nperm=nperm, alpha=0.10 )

par(mfrow = c(1, 2))
plot_quantile_CIs(ci6, main="s=6", k_start = n-37)
plot_quantile_CIs(ci2, main="s=2", k_start = n-37)
```

```{r, include=FALSE}
save.image("Homefood20220921.RData") # save the analysis result
```


